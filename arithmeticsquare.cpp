/*You are given a 3×3 grid of integers. Let Gi,j denote the integer in the i-th row and j-th column of the grid, where i and j are 0-indexed. The integer in the middle of the grid, G1,1, is missing. Find the maximum number of rows, columns, and diagonals of this square, that form sequences which are arithmetic progressions. You can replace the missing number with any integer.

An arithmetic progression (also known as arithmetic sequence) is a sequence of numbers such that the difference between consecutive terms is constant. In mathematical terms, this can be represented as an=an−1+d, where d is the common difference. In this problem, a sequence can be the 3 numbers in either a row, column or diagonal. We are looking to replace the missing value by an integer that maximizes the number of arithmetic progressions that can be found in the resulting set of sequences.

Two sequences are considered different if they are from different rows, columns, or diagonals. For example, the sequence {2,4,6} across the middle row and {2,4,6} across the top row will be counted as two sequences but the sequences {2,4,6} and {6,4,2} across the same row, column, or diagonal will be counted as one sequence.

Input
The first line of the input gives the number of test cases, T. T test cases follow.
Each test case consists of 3 lines.
The first line of each test case contains 3 integers, G0,0, G0,1, and G0,2.
The second line of each test case contains 2 integers, G1,0 and G1,2.
The last line of each test case contains 3 integers, G2,0, G2,1, and G2,2.
Output
For each test case, output one line containing Case #x: y, where x is the test case number (starting from 1) and y is the maximum possible number of arithmetic progressions that can be generated by the rows, columns, and diagonals of the grid after setting the missing element.

Limits
Memory limit: 1 GB.
1≤T≤100.
Gi,j are integers, for all i,j.

Test Set 1
Time limit: 20 seconds.
|Gi,j|≤50, for all i,j.
Test Set 2
Time limit: 40 seconds.
|Gi,j|≤109, for all i,j.*/
//commented lines are for debugging

#include <bits/stdc++.h>

int main()
{
    std::ios_base::sync_with_stdio(0);
    std::cin.tie(NULL);
    int t;
    std::cin >> t;
    for (int k = 1; k <= t; ++k)
    {
        std::cout << "Case #" << k << ": ";
        int arr[3][3], mean[4];
        int apcount = 0, i = 0, j = 0, count = 0, occurance = 0;
        for (i = 0; i < 3; ++i)
            for (j = 0; j < 3; ++j)
            {
                if ((i == 1) && (j == 1))
                    continue;
                std::cin >> arr[i][j];
            }
        for (i = 0; i < 3; i = i + 2)
        {
            j = 0;
            if ((arr[i][j] + arr[i][j + 2]) == 2 * arr[i][j + 1])
                ++apcount;
        }
        //std::cout << "appcount:" << apcount << std::endl; 
        for (j = 0; j < 3; j = j + 2)
        {
            i = 0;
            if ((arr[i][j] + arr[i + 2][j]) == 2 * arr[i + 1][j])
                ++apcount;
        }
        //std::cout << "appcount:" << apcount << std::endl;
        int s = 0;
        if (((arr[0][0] + arr[2][2]) % 2) == 0)
        {
            mean[s] = (arr[0][0] + arr[2][2]) / 2;
            ++s;
        }
        if (((arr[1][0] + arr[1][2]) % 2) == 0)
        {
            mean[s] = (arr[1][0] + arr[1][2]) / 2;
            ++s;
        }
        if (((arr[2][0] + arr[0][2]) % 2) == 0)
        {
            mean[s] = (arr[2][0] + arr[0][2]) / 2;
            ++s;
        }
        if (((arr[0][1] + arr[2][1]) % 2) == 0)
        {
            mean[s] = (arr[0][1] + arr[2][1]) / 2;
            ++s;
        }
        /*for (i = 0; i < s; ++i)
            std::cout << mean[i] << std::endl;
        std::cout << s << std::endl;*/

        for (i = 0; i < s; ++i)
        {

            count = 0;
            for (j = 0; j < s; ++j)
            {

                if (mean[i] == mean[j])
                    ++count;
            }
            if (occurance < count)
                occurance = count;
        }
        std::cout << apcount + occurance << "\n";
    }
    return 0;
}
